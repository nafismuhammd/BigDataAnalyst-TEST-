{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lembar Kerja UTS | MK Analisis Big Data Kelas A \n",
    "Fakultas Ilmu Komputer (FILKOM), Universitas Brawijaya (UB) 2020 | Dosen Pengampu: Imam Cholissodin, S.Si., M.Kom. \n",
    "\n",
    "Nama Mahasiswa: Muhammad Nafis | NIM: 175150200111008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Kerjakan soal No. 1 sampai 10 pada (click) [Cell Utk Tempat Koding](#cell_utk_koding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: Apache Spark Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Konfigurasi utk konek ke Spark cluster, menggunakan SparkSession object dengan parameter berikut:\n",
    "\n",
    "+ **appName:** set nama app yang akan ditampilkan pada [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** set Spark Master URL, yang sudah terkoneksi dengan Spark Workers;\n",
    "+ **spark.executor.memory:** set paling tidak <= dari set pada SPARK_WORKER_MEMORY config pada docker-compose.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-koding-uts\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cell_utk_koding_'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRDD : \n",
      "[['8.0', '9.1', 'good'], ['7.8', '9.4', 'good'], ['12.5', '15.3', 'great'], ['13.4', '16.3', 'great'], ['12.1', '15.7', 'great']]\n",
      "+----------+----------+-----+\n",
      "|Fitur ke-1|Fitur ke-2|Kelas|\n",
      "+----------+----------+-----+\n",
      "|       8.0|       9.1| good|\n",
      "|       7.8|       9.4| good|\n",
      "|      12.5|      15.3|great|\n",
      "|      13.4|      16.3|great|\n",
      "|      12.1|      15.7|great|\n",
      "+----------+----------+-----+\n",
      "\n",
      "Data frek tiap kelas : \n",
      "+-----+---------------+\n",
      "|Kelas|Frek tiap kelas|\n",
      "+-----+---------------+\n",
      "| good|            2.0|\n",
      "|great|            3.0|\n",
      "+-----+---------------+\n",
      "\n",
      "Prior tiap kelas : \n",
      "+-----+-----+\n",
      "|Kelas|Prior|\n",
      "+-----+-----+\n",
      "| good|  0.4|\n",
      "|great|  0.6|\n",
      "+-----+-----+\n",
      "\n",
      "DataTest\n",
      "[('1', '8.43', '15.0')]\n",
      "DataTestDF\n",
      "+---+----+----+\n",
      "| _1|  _2|  _3|\n",
      "+---+----+----+\n",
      "|  1|8.43|15.0|\n",
      "+---+----+----+\n",
      "\n",
      "[('8.43', '15.0')]\n",
      "Fitur ke-1: 8.43\n",
      "Mean:  7.9\n",
      "Standar Deviasi:  0.14142135623730964\n",
      "P( 8.43 | good )= 0.0025164650581969887\n",
      "\n",
      "Mean:  12.666666666666666\n",
      "Standar Deviasi:  0.6658328118479396\n",
      "P( 8.43 | great )= 9.697860090897304e-10\n",
      "\n",
      "Fitur ke-2: 15.0\n",
      "Mean:  9.25\n",
      "Standar Deviasi:  0.21213203435596475\n",
      "P( 15.0 | good )= 5.5957086368821125e-160\n",
      "\n",
      "Mean:  15.766666666666666\n",
      "Standar Deviasi:  0.5033222956847168\n",
      "P( 15.0 | great )= 0.24843286977389645\n",
      "\n",
      "Evidence:  1.0\n",
      "\n",
      "Menampilkan hasil kelas dataTest\n",
      "Final P_posterior ( good | data Test ): 5.632562104225975e-163\n",
      "Final P_posterior ( great | data Test ): 1.4455603278284143e-10\n",
      "\n",
      "Jadi data test :  [('8.43', '15.0')] masuk pada kelas : great\n"
     ]
    }
   ],
   "source": [
    "dataRDD = sc.textFile(\"data.csv\") \\\n",
    "    .map(lambda line: line.split(\";\")) \\\n",
    "    .zipWithIndex().filter(lambda baris: baris[1] > 0) \\\n",
    "    .map(lambda x:x[0]) \\\n",
    "    .map(lambda x:x[1:])\n",
    "print(\"DataRDD : \")\n",
    "tampil = dataRDD.collect()\n",
    "print(tampil)\n",
    "\n",
    "isidataRDD = spark.createDataFrame(dataRDD).toDF(\"Fitur ke-1\", \"Fitur ke-2\",\"Kelas\")\n",
    "isidataRDD.show()\n",
    "\n",
    "total_dataTrain = isidataRDD.count()\n",
    "FrekTiapKelasRDD = dataRDD.map(lambda x:(x[2],1.0)).reduceByKey(lambda x,y:x+y)\n",
    "dataDFfrekKelas = spark.createDataFrame(FrekTiapKelasRDD).toDF(\"Kelas\",\"Frek tiap kelas\")\n",
    "print(\"Data frek tiap kelas : \")\n",
    "dataDFfrekKelas.show()\n",
    "\n",
    "priorRDD = FrekTiapKelasRDD.map(lambda x:(x[0],x[1]/total_dataTrain))\n",
    "priorDF = spark.createDataFrame(priorRDD).toDF(\"Kelas\",\"Prior\")\n",
    "print(\"Prior tiap kelas : \")\n",
    "priorDF.show()\n",
    "\n",
    "dataTestRDD = sc.parallelize([('1','8.43','15.0')])\n",
    "print(\"DataTest\")\n",
    "print(dataTestRDD.collect())\n",
    "print(\"DataTestDF\")\n",
    "dataDF = spark.createDataFrame(dataTestRDD2)\n",
    "dataDF.show()\n",
    "\n",
    "\n",
    "byk_fitur = 2\n",
    "fitur = []\n",
    "kelas = [\"good\", \"great\"]\n",
    "byk_kelas = len(kelas)\n",
    "p_good = 1.\n",
    "p_great = 1.\n",
    "evi_good = 1.\n",
    "evi_great = 1.\n",
    "evidence = 1.\n",
    "\n",
    "P_posterior = [1.,1.]\n",
    "for m in range(0,1):\n",
    "    oneDataTestRDD = dataTestRDD.zipWithIndex().filter(lambda baris:baris[1]==m).map(lambda x:x[0][1:3])\n",
    "    print(oneDataTestRDD.collect())\n",
    "    for i in range(0,byk_fitur):\n",
    "        fitur.append(oneDataTestRDD.map(lambda x:x[i]).collect()[0])\n",
    "        print(\"Fitur ke-\"+str(i+1)+\": \"+ fitur[i])\n",
    "        \n",
    "        for j in range(0,byk_kelas):\n",
    "            #menghitung mean dan standar deviasi untuk persiapan perhitungan likelihood\n",
    "            mean = dataRDD.filter(lambda x:x[2]==kelas[j]).map(lambda x:float(x[i])).reduce(lambda x,y:x+y)/\\\n",
    "                    FrekTiapKelasRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0]\n",
    "            frekKelas = FrekTiapKelasRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0]\n",
    "            std = dataRDD.filter(lambda x:x[2]==kelas[j]).map(lambda x:((float(x[i])-mean)**2)/(frekKelas-1)).reduce(lambda x,y:x+y)**0.5\n",
    "            \n",
    "            #no 2, kode program menghitung nilai probabilitas likelihood\n",
    "            myFnLikelihood = (lambda x:(1/( (2*(22/7)*(x[2])**2)**(0.5) ))*( 2.718**(-(((x[0]-x[1])**2)/(2*x[2]**2)))))\n",
    "            contohDataRDD = sc.parallelize([(float(fitur[i]),mean,std)])\n",
    "            likelihood = contohDataRDD.map(lambda x:myFnLikelihood(x)).collect()[0]\n",
    "            print(\"Mean: \", mean)\n",
    "            print(\"Standar Deviasi: \", std)\n",
    "            print(\"P(\",fitur[i],\"|\",kelas[j],\")=\",likelihood)\n",
    "            \n",
    "            P_posterior[j]*=likelihood\n",
    "            \n",
    "            print()\n",
    "            #seleksi kondisi untuk persiapan perhitungan evidence\n",
    "            if(kelas[j]==\"Good\"):\n",
    "                p_good *= likelihood\n",
    "                evi_good = p_good * float(priorRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0])\n",
    "            else:\n",
    "                p_great *= likelihood\n",
    "                evi_great = p_great * float(priorRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0])\n",
    "                \n",
    "#  p(x) = [p(x1|good) * p(x2|good) * p(good)] + [p(x1|great) * p(x2|great) * p(great)]\n",
    "evidence = evi_good + evi_great\n",
    "print(\"Evidence: \", evidence)\n",
    "P_posteriorRDD = sc.parallelize(P_posterior)\n",
    "    \n",
    "print()\n",
    "print('Menampilkan hasil kelas dataTest')\n",
    "for j in range(0,byk_kelas):\n",
    "    P_posterior[j]*= priorRDD.filter(lambda x:x[0]==kelas[j]).map(lambda x:x[1]).collect()[0] / evidence\n",
    "    print(\"Final P_posterior (\",kelas[j],\"| data Test ):\", P_posterior[j])\n",
    "\n",
    "print()\n",
    "\n",
    "Argmax = P_posteriorRDD.zipWithIndex().sortBy(lambda x:x[0], ascending=False).map(lambda x:x[1]).collect()[0]    \n",
    "\n",
    "print(\"Jadi data test : \", oneDataTestRDD.collect(),\"masuk pada kelas :\", kelas[Argmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saya', '#Dia', 'kamu', '#kalian', '#dia', '#dia', 'dan', '#kalian', 'adalah', '#saudara', '#kamu', '#dia', 'dan', '#SaYA', 'serta', 'kalian', 'bersaudara']\n",
      "\n",
      "Hasil : \n",
      "[('#saudara', 1), ('#saya', 1), ('#kamu', 1), ('#dia', 4), ('#kalian', 2)]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.parallelize([(\"ID-Twit-01\",\"saya #Dia kamu #kalian #dia #dia dan #kalian adalah #saudara\"),\\\n",
    "                        (\"ID-Twit-02\",\"#kamu #dia dan #SaYA serta kalian bersaudara\")])\n",
    "\n",
    "hashtags = lines.map(lambda x:x[1]).flatMap(lambda text: text.split(\" \"))\n",
    "print(hashtags.collect())\n",
    "\n",
    "toLowerCase = hashtags.map(lambda x:(x.lower(),1)).reduceByKey(lambda x,y:x+y).filter(lambda kata: '#' in kata[0])\n",
    "print(\"\\nHasil : \")\n",
    "print(toLowerCase.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+-------------------+-------------------+\n",
      "|DocumentId|     Token| TF|                IDF|             TF-IDF|\n",
      "+----------+----------+---+-------------------+-------------------+\n",
      "|         1|     cinta|  1|0.47712125471966244|0.47712125471966244|\n",
      "|         1|       aku|  1|0.17609125905568124|0.17609125905568124|\n",
      "|         1| indonesia|  1|0.17609125905568124|0.17609125905568124|\n",
      "|         2|       aku|  2|0.17609125905568124| 0.3521825181113625|\n",
      "|         2|permusuhan|  1|0.47712125471966244|0.47712125471966244|\n",
      "|         2|      anti|  2|0.47712125471966244| 0.9542425094393249|\n",
      "|         2|perpecahan|  1|0.47712125471966244|0.47712125471966244|\n",
      "|         3|     dasar|  1|0.47712125471966244|0.47712125471966244|\n",
      "|         3| pancasila|  1|0.47712125471966244|0.47712125471966244|\n",
      "|         3| indonesia|  1|0.17609125905568124|0.17609125905568124|\n",
      "|         3|    negara|  1|0.47712125471966244|0.47712125471966244|\n",
      "+----------+----------+---+-------------------+-------------------+\n",
      "\n",
      "['anti', 'perpecahan', 'cinta', 'dasar', 'pancasila', 'aku', 'indonesia', 'negara', 'permusuhan']\n",
      "[(1, 0.47712125471966244), (2, 0.9542425094393249), (3, 0.47712125471966244)]\n",
      "Data TF-IDF terbesar :\n",
      "+-------+-------------------+\n",
      "|Data id|     Nilai Terbesar|\n",
      "+-------+-------------------+\n",
      "|      1|0.47712125471966244|\n",
      "|      2| 0.9542425094393249|\n",
      "|      3|0.47712125471966244|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "data=[(1,'aku cinta Indonesia'),(2,\"Aku anti Perpecahan dan aku anti permusuhan\"), \\\n",
    "      (3,\"Pancasila adalah dasar negara indonesia\")]\n",
    "lines=sc.parallelize(data)\n",
    "\n",
    "hapusword = ['dan', 'Adalah']\n",
    "hapusword_lower = [x.lower() for x in hapusword]\n",
    "\n",
    "# ((x[0],i),1) merupakan kombinasi (\"id dokumen\", \"term\", dan set nilai 1 utk tiap term)\n",
    "map1=lines.flatMap(lambda x: [((x[0],i),1) for i in x[1].split() if i.lower() not in hapusword_lower])\n",
    "reduce=map1.map(lambda term:((term[0][0],term[0][1].lower()),term[1])).reduceByKey(lambda x,y:x+y)\n",
    "tf=reduce.map(lambda x: (x[0][1],(x[0][0],x[1])))\n",
    "\n",
    "map3=reduce.map(lambda x: (x[0][1],(x[0][0],x[1],1)))\n",
    "map3.collect()\n",
    "map4=map3.map(lambda x:(x[0],x[1][2]))\n",
    "map4.collect()\n",
    "reduce2=map4.reduceByKey(lambda x,y:x+y)\n",
    "reduce2.collect()\n",
    "idf=reduce2.map(lambda x: (x[0],math.log10(len(data)/x[1])))\n",
    "idf.collect()\n",
    "\n",
    "rdd=tf.join(idf)\n",
    "rdd=rdd.map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1]))).sortByKey()\n",
    "rdd.collect()\n",
    "rdd=rdd.map(lambda x: (x[0],x[1][0],x[1][1],x[1][2],x[1][3]))\n",
    "\n",
    "rdd.toDF([\"DocumentId\",\"Token\",\"TF\",\"IDF\",\"TF-IDF\"]).show()\n",
    "\n",
    "rddDistinct = rdd.map(lambda x:x[1]).distinct()\n",
    "# rddDistinct = rdd.map(lambda x:x[1])\n",
    "byk_fitur = rddDistinct.count()\n",
    "print(rddDistinct.collect())\n",
    "\n",
    "nilaiMaks = rdd.map(lambda x:(x[0],x[4])).reduceByKey(lambda x,y:max(x,y))\n",
    "print(nilaiMaks.collect())\n",
    "\n",
    "print(\"Data TF-IDF terbesar :\")\n",
    "makstoDF = spark.createDataFrame(nilaiMaks).toDF(\"Data id\",\"Nilai Terbesar\")\n",
    "makstoDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.1-1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "--2020-11-18 02:39:29--  https://docs.google.com/uc?export=download&confirm=9Bp8&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.9.206, 2607:f8b0:4004:811::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.9.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download [following]\n",
      "--2020-11-18 02:39:29--  https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download\n",
      "Resolving doc-0s-3g-docs.googleusercontent.com (doc-0s-3g-docs.googleusercontent.com)... 172.217.164.161, 2607:f8b0:4004:807::2001\n",
      "Connecting to doc-0s-3g-docs.googleusercontent.com (doc-0s-3g-docs.googleusercontent.com)|172.217.164.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=ogsbp37o03fte&continue=https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e%3Ddownload&hash=ja7t9jnt3kj1rvl7j4bic7i4rknrpssk [following]\n",
      "--2020-11-18 02:39:29--  https://docs.google.com/nonceSigner?nonce=ogsbp37o03fte&continue=https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e%3Ddownload&hash=ja7t9jnt3kj1rvl7j4bic7i4rknrpssk\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.9.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download&nonce=ogsbp37o03fte&user=17838546180223045918Z&hash=gucp0e1qf9374kb5l535o1skpn45flp8 [following]\n",
      "--2020-11-18 02:39:29--  https://doc-0s-3g-docs.googleusercontent.com/docs/securesc/fk6kv7a9pnosa987t9j5c6u65vcjj3f6/7ven6sc61dvc4malkujamrvapg1mueqb/1605667125000/09783780208665738920/17838546180223045918Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download&nonce=ogsbp37o03fte&user=17838546180223045918Z&hash=gucp0e1qf9374kb5l535o1skpn45flp8\n",
      "Connecting to doc-0s-3g-docs.googleusercontent.com (doc-0s-3g-docs.googleusercontent.com)|172.217.164.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘tweet.csv’\n",
      "\n",
      "tweet.csv               [   <=>              ]   1.19G  51.7MB/s    in 16s     \n",
      "\n",
      "2020-11-18 02:39:45 (78.2 MB/s) - ‘tweet.csv’ saved [1276813940]\n",
      "\n",
      "18\n",
      "Menampilkan sebagian dataRDD:\n",
      "[['2019-07-22 05:41:47', '1.1531772532341473e+18', '1153178304603545600', 'False', '731167584.0', '', '0', '', '0', 'False', '1.1531772532341473e+18', 'Twitter for iPhone', '731167584.0', '1003356794683416576', 'CatsRule98', '', '', '@RonBrownstein That’s why #IStandWithIlhan'], ['2019-07-22 05:41:48', '', '1153178308776869888', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '1097334060', 'JejakaShahid', '', '', 'RT @mathsanova: mana pakai faceapp lagi 🤪🤣 https://t.co/pO4adR5dI6'], ['2019-07-22 05:41:49', '', '1153178311515766785', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '1014117841', 'nurlailaeksa', '', '', '\"RT @BadmintonTalk: #BtalkBWFRankPrediction MS after #BlibliIndonesiaOpen2019 and #RussianOpenSuper100 '], ['2019-07-22 05:41:49', '', '1153178313948426241', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '701611356496113664', 'sticvh', '', '', 'RT @Husen_Jafar: FaceApp haram? #RecehanDakwah https://t.co/uOljFfVN1O'], ['2019-07-22 05:42:52', '', '1153178575454912514', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '244026883', 'its_Mero_', '', '', 'RT @mathsanova: mana pakai faceapp lagi 🤪🤣 https://t.co/pO4adR5dI6'], ['2019-07-22 05:42:59', '', '1153178605486137344', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for iPhone', '', '21052894', 'kefish', '', '', 'RT @NPR: Sen. Chuck Schumer has called for a federal investigation into the Russia-based company FaceApp over what he says are potential na…'], ['2019-07-22 05:42:59', '', '1153178607600177152', 'False', '', '', '0', '', '0', 'False', '', 'TweetDeck', '', '3279869646', 'diabolik_heart', '', '', 'RT @JoeBrogie: This FaceApp thing is getting out of hand... Thanks to @NintendoAmerica  for letting me play #Sylvain in the new #fireemblem…']]\n",
      "\n",
      "Banyak baris pada data tweet.csv =  4147825  baris data twitter tanpa malform\n",
      "Waktu load data dan penghitungan banyak data =  40.8495907769975  detik\n",
      "Menampilkan data username dan jumlahnya\n",
      "+---------------+------+\n",
      "|           Name|Jumlah|\n",
      "+---------------+------+\n",
      "|     wordnuvola|  2208|\n",
      "| GooglePayIndia|  2112|\n",
      "|       Weinbach|  1808|\n",
      "|       WwuRadio|  1179|\n",
      "|        pogosj1|  1158|\n",
      "|Christo73106853|  1112|\n",
      "|lililin79416363|   933|\n",
      "|MercadoLechugas|   917|\n",
      "|mars_project_on|   791|\n",
      "|     dpdcentral|   704|\n",
      "|         56_aae|   681|\n",
      "|    iamtaquirah|   677|\n",
      "|     Sravslucky|   669|\n",
      "|    RBitterwolf|   656|\n",
      "|      4oesmomma|   642|\n",
      "|     xxzzxxzzzz|   641|\n",
      "|   SteveSpeedZA|   638|\n",
      "|  jeffdavisshow|   635|\n",
      "|     AqsaEshaal|   610|\n",
      "|   DinaElHakim4|   608|\n",
      "+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Menampilkan 10 teratas tweet favorit\n",
      "+---------------+------+\n",
      "|           Name|jumlah|\n",
      "+---------------+------+\n",
      "|     wordnuvola|  2208|\n",
      "| GooglePayIndia|  2112|\n",
      "|       Weinbach|  1808|\n",
      "|       WwuRadio|  1179|\n",
      "|        pogosj1|  1158|\n",
      "|Christo73106853|  1112|\n",
      "|lililin79416363|   933|\n",
      "|MercadoLechugas|   917|\n",
      "|mars_project_on|   791|\n",
      "|     dpdcentral|   704|\n",
      "+---------------+------+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  6|  4|\n",
      "|  2|  3|\n",
      "|  4|  2|\n",
      "|  8|  1|\n",
      "+---+---+\n",
      "\n",
      "Menampilkan tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "|RT @BTS_twt: Seou...| 54910|\n",
      "|\"RT @BTS_twt: 😱😱💜| 54180|\n",
      "|RT @realDonaldTru...| 19948|\n",
      "|RT @TheXFactor: W...| 16612|\n",
      "|RT @LeftAtLondon:...| 16298|\n",
      "|\"RT @BigHitEnt: S...| 14825|\n",
      "|RT @JoshDevineDru...| 14374|\n",
      "|\"RT @hendralm: Se...| 14233|\n",
      "|RT @PhilPrajya: ร...| 14006|\n",
      "|\"RT @Abdulaziz_Hm...| 13333|\n",
      "|RT @hellofeb22: ถ...| 13279|\n",
      "|\"RT @NCTsmtown_DR...| 12026|\n",
      "|RT @Lazy_and_lame...| 10438|\n",
      "|\"RT @Pantene: I G...| 10437|\n",
      "|RT @aserembaX: Na...|  9476|\n",
      "|RT @chartdata: .@...|  8936|\n",
      "|\"RT @Believe_In_L...|  8228|\n",
      "|RT @mistxxke: for...|  8096|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Menampilkan 2 tweet terbanyak:\n",
      "[('RT @LiamPayne: 9 years... Amazing to look back at all the memories and even now you got us trending worldwide after 9 years. You truly are…', 133763), ('RT @Zayg23: I cracked an egg and it was double yolked! I was excited so I recorded it on snap and asked google what it mean  ...  Should’ve…', 131023)]\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "+--------------------+------+\n",
      "\n",
      "Menampilkan tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "|RT @BTS_twt: Seou...| 54910|\n",
      "|\"RT @BTS_twt: 😱😱💜| 54180|\n",
      "|RT @realDonaldTru...| 19948|\n",
      "|RT @TheXFactor: W...| 16612|\n",
      "|RT @LeftAtLondon:...| 16298|\n",
      "|\"RT @BigHitEnt: S...| 14825|\n",
      "|RT @JoshDevineDru...| 14374|\n",
      "|\"RT @hendralm: Se...| 14233|\n",
      "|RT @PhilPrajya: ร...| 14006|\n",
      "|\"RT @Abdulaziz_Hm...| 13333|\n",
      "|RT @hellofeb22: ถ...| 13279|\n",
      "|\"RT @NCTsmtown_DR...| 12026|\n",
      "|RT @Lazy_and_lame...| 10438|\n",
      "|\"RT @Pantene: I G...| 10437|\n",
      "|RT @aserembaX: Na...|  9476|\n",
      "|RT @chartdata: .@...|  8936|\n",
      "|\"RT @Believe_In_L...|  8228|\n",
      "|RT @mistxxke: for...|  8096|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Menampilkan 2 tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "+--------------------+------+\n",
      "\n",
      "[(('🧸💞 https://t.co/2Fxhd6GSSo', False), 1), (('🧸💕 https://t.co/xhnYD7ZAMW', False), 1), (('🧸ARMY are extremely dedicated and think #BTS is top tier! We really want to hear #SeoulTownRoad by @LilNasX feat RM… https://t.co/dAxEW2CCDm', False), 1), (('🧸 https://t.co/vZUSTP24D7', False), 1), (('🧸 https://t.co/rvTZSnE4OE', False), 1)]\n"
     ]
    }
   ],
   "source": [
    "# https://drive.google.com/file/d/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk/view?usp=sharing\n",
    "#install wget terlebih dahulu\n",
    "!echo y | apt-get install wget\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk\" -O 'tweet.csv' && rm -rf /tmp/cookies.txt\n",
    "\n",
    "nama_header = ['created_at', 'in_reply_to_status_id', 'id_str', \\\n",
    " 'retweeted', 'in_reply_to_user_id_str', 'coordinates', \\\n",
    " 'retweet_count', 'contributors', 'favorite_count', \\\n",
    " 'favorited', 'in_reply_to_status_id_str', 'source', \\\n",
    " 'in_reply_to_user_id', 'user_id_str', 'user_screen_name', \\\n",
    " 'place', 'geo', 'text']\n",
    "\n",
    "byk_kolom= len(nama_header)\n",
    "print(byk_kolom)\n",
    "\n",
    "# Load Dataset\n",
    "start = timer()\n",
    "dataRDD = sc.textFile(\"tweet.csv\") \\\n",
    "    .filter(lambda line: line.count(\",\") == byk_kolom-1 ) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .zipWithIndex().filter(lambda baris: baris[1] > 0) \\\n",
    "    .map(lambda x:x[0])\n",
    "\n",
    "print(\"Menampilkan sebagian dataRDD:\")\n",
    "print(dataRDD.take(7))\n",
    "print()\n",
    "print(\"Banyak baris pada data tweet.csv = \",dataRDD.count(),\" baris data twitter tanpa malform\")\n",
    "end = timer()\n",
    "print(\"Waktu load data dan penghitungan banyak data = \", end - start, \" detik\")\n",
    "#print(timedelta(seconds=end-start))\n",
    "\n",
    "#Menampilkan 10 teratas tweet favorit\n",
    "dataRDD_fav = dataRDD.map(lambda x:(x[14],1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False)\n",
    "print(\"Menampilkan data username dan jumlahnya\")\n",
    "data_favtoDF = spark.createDataFrame(dataRDD_fav).toDF(\"Name\",\"Jumlah\")\n",
    "data_favtoDF.show()\n",
    "print(\"Menampilkan 10 teratas tweet favorit\")\n",
    "dataRDD_fav10 = dataRDD_fav.take(10)\n",
    "dataRDD_fav10DF = spark.createDataFrame(dataRDD_fav10).toDF(\"Name\",\"jumlah\")\n",
    "dataRDD_fav10DF.show()\n",
    "\n",
    "temp = sc.parallelize([(4,2), (8,1), (6,4), (2,3)])\n",
    "sort1 = temp.sortBy(lambda x:x[1], ascending=False)\n",
    "dataDF = spark.createDataFrame(sort1)\n",
    "dataDF.show()\n",
    "\n",
    "jumlahTwet = dataRDD.map(lambda x:(x[17], 1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], ascending=False)\n",
    "\n",
    "print(\"Menampilkan tweet terbanyak:\")\n",
    "dataDF = spark.createDataFrame(jumlahTwet).toDF(\"Tweet\", \"Jumlah\")\n",
    "dataDF.show()\n",
    "\n",
    "print(\"Menampilkan 2 tweet terbanyak:\")\n",
    "jumlahTwet2 = jumlahTwet.take(2)\n",
    "print(jumlahTwet2)\n",
    "dataDF = spark.createDataFrame(jumlahTwet2).toDF(\"Tweet\", \"Jumlah\")\n",
    "dataDF.show()\n",
    "\n",
    "jumlahTwet = dataRDD.map(lambda x:(x[17], 1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], ascending=False)\n",
    "print(\"Menampilkan tweet terbanyak:\")\n",
    "dataDF = spark.createDataFrame(jumlahTwet).toDF(\"Tweet\", \"Jumlah\")\n",
    "dataDF.show()\n",
    "print(\"Menampilkan 2 tweet terbanyak:\")\n",
    "jumlahTwet2 = jumlahTwet.take(2)\n",
    "dataDF = spark.createDataFrame(jumlahTwet2).toDF(\"Tweet\", \"Jumlah\")\n",
    "dataDF.show()\n",
    "\n",
    "dataRDD_text_plg_favorit_alt2 = dataRDD.map(lambda x:((x[17],x[9]=='True'),1)).reduceByKey(lambda x,y:x+y)\\\n",
    ".sortBy(lambda x:x,ascending=False).take(5)\n",
    "print(dataRDD_text_plg_favorit_alt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Petunjuk **koding PySpark untuk UTS**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.1-1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-14 15:42:21--  https://docs.google.com/uc?export=download&confirm=sCHY&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.8.14, 2607:f8b0:4004:811::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.8.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download [following]\n",
      "--2020-11-14 15:42:21--  https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download\n",
      "Resolving doc-0g-4g-docs.googleusercontent.com (doc-0g-4g-docs.googleusercontent.com)... 172.217.13.65, 2607:f8b0:4004:807::2001\n",
      "Connecting to doc-0g-4g-docs.googleusercontent.com (doc-0g-4g-docs.googleusercontent.com)|172.217.13.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=mig5qdsvk68b8&continue=https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e%3Ddownload&hash=9c26prn681dihl4stt3ucdq75ktn0mgk [following]\n",
      "--2020-11-14 15:42:21--  https://docs.google.com/nonceSigner?nonce=mig5qdsvk68b8&continue=https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e%3Ddownload&hash=9c26prn681dihl4stt3ucdq75ktn0mgk\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.8.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download&nonce=mig5qdsvk68b8&user=10219236717386092432Z&hash=ffu862mbm351me2pg36f6esmvfc63aem [following]\n",
      "--2020-11-14 15:42:21--  https://doc-0g-4g-docs.googleusercontent.com/docs/securesc/qnmh9no26unvrnju3jeo5tqv1m9i455h/ca1b6j08m8ft9r09ui4cjauplegio6k5/1605368475000/09783780208665738920/10219236717386092432Z/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk?e=download&nonce=mig5qdsvk68b8&user=10219236717386092432Z&hash=ffu862mbm351me2pg36f6esmvfc63aem\n",
      "Connecting to doc-0g-4g-docs.googleusercontent.com (doc-0g-4g-docs.googleusercontent.com)|172.217.13.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘tweet.csv’\n",
      "\n",
      "tweet.csv               [                <=> ]   1.19G  20.0MB/s    in 53s     \n",
      "\n",
      "2020-11-14 15:43:15 (22.9 MB/s) - ‘tweet.csv’ saved [1276813940]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# utk download data tweet.csv dari link public G Drive\n",
    "# https://drive.google.com/file/d/1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk/view?usp=sharing\n",
    "\n",
    "# lakukan instalasi dulu wget dgn \"!echo y | apt-get install wget\"\n",
    "#!echo y | apt-get install wget\n",
    "\n",
    "# cara download-nya sbg berikut:\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1YVHgU7TXgFc_u6tMqQWJiXBR4Yf4_NQk\" -O 'tweet.csv' && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1218M Nov 14 15:43 ./tweet.csv\n"
     ]
    }
   ],
   "source": [
    "#cek ukuran file \"tweet.csv\"\n",
    "!ls ./tw* -a -l --block-size=m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan sebagian dataRDD:\n",
      "[['2019-07-22 05:41:47', '1.1531772532341473e+18', '1153178304603545600', 'False', '731167584.0', '', '0', '', '0', 'False', '1.1531772532341473e+18', 'Twitter for iPhone', '731167584.0', '1003356794683416576', 'CatsRule98', '', '', '@RonBrownstein That’s why #IStandWithIlhan'], ['2019-07-22 05:41:48', '', '1153178308776869888', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '1097334060', 'JejakaShahid', '', '', 'RT @mathsanova: mana pakai faceapp lagi 🤪🤣 https://t.co/pO4adR5dI6'], ['2019-07-22 05:41:49', '', '1153178311515766785', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '1014117841', 'nurlailaeksa', '', '', '\"RT @BadmintonTalk: #BtalkBWFRankPrediction MS after #BlibliIndonesiaOpen2019 and #RussianOpenSuper100 '], ['2019-07-22 05:41:49', '', '1153178313948426241', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '701611356496113664', 'sticvh', '', '', 'RT @Husen_Jafar: FaceApp haram? #RecehanDakwah https://t.co/uOljFfVN1O'], ['2019-07-22 05:42:52', '', '1153178575454912514', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for Android', '', '244026883', 'its_Mero_', '', '', 'RT @mathsanova: mana pakai faceapp lagi 🤪🤣 https://t.co/pO4adR5dI6'], ['2019-07-22 05:42:59', '', '1153178605486137344', 'False', '', '', '0', '', '0', 'False', '', 'Twitter for iPhone', '', '21052894', 'kefish', '', '', 'RT @NPR: Sen. Chuck Schumer has called for a federal investigation into the Russia-based company FaceApp over what he says are potential na…'], ['2019-07-22 05:42:59', '', '1153178607600177152', 'False', '', '', '0', '', '0', 'False', '', 'TweetDeck', '', '3279869646', 'diabolik_heart', '', '', 'RT @JoeBrogie: This FaceApp thing is getting out of hand... Thanks to @NintendoAmerica  for letting me play #Sylvain in the new #fireemblem…']]\n",
      "\n",
      "Banyak baris pada data tweet.csv =  4147825  baris data twitter tanpa malform\n",
      "Waktu load data dan penghitungan banyak data =  196.66930147493258  detik\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan data username dan jumlahnya\n",
      "+---------------+------+\n",
      "|           Name|Jumlah|\n",
      "+---------------+------+\n",
      "|     wordnuvola|  2208|\n",
      "| GooglePayIndia|  2112|\n",
      "|       Weinbach|  1808|\n",
      "|       WwuRadio|  1179|\n",
      "|        pogosj1|  1158|\n",
      "|Christo73106853|  1112|\n",
      "|lililin79416363|   933|\n",
      "|MercadoLechugas|   917|\n",
      "|mars_project_on|   791|\n",
      "|     dpdcentral|   704|\n",
      "|         56_aae|   681|\n",
      "|    iamtaquirah|   677|\n",
      "|     Sravslucky|   669|\n",
      "|    RBitterwolf|   656|\n",
      "|      4oesmomma|   642|\n",
      "|     xxzzxxzzzz|   641|\n",
      "|   SteveSpeedZA|   638|\n",
      "|  jeffdavisshow|   635|\n",
      "|     AqsaEshaal|   610|\n",
      "|   DinaElHakim4|   608|\n",
      "+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Manmpilkan 10 teratas tweet favorit\n",
      "+---------------+------+\n",
      "|           Name|jumlah|\n",
      "+---------------+------+\n",
      "|     wordnuvola|  2208|\n",
      "| GooglePayIndia|  2112|\n",
      "|       Weinbach|  1808|\n",
      "|       WwuRadio|  1179|\n",
      "|        pogosj1|  1158|\n",
      "|Christo73106853|  1112|\n",
      "|lililin79416363|   933|\n",
      "|MercadoLechugas|   917|\n",
      "|mars_project_on|   791|\n",
      "|     dpdcentral|   704|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buatlah kode program Spark RDD atau Spark SQL untuk mendapatkan 10 “text” \n",
    "# yaitu Tweet apa yang paling difavoritkan dan user_screen_name-nya?\n",
    "# dataRDD_text_plg_favorit_alt1 = dataRDD.map(lambda x:(x[17],int(x[8]))).reduceByKey(lambda x,y:x+y)\\\n",
    "# .sortBy(lambda x:x[1],ascending=False).take(5)\n",
    "# print(dataRDD_text_plg_favorit_alt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  6|  4|\n",
      "|  2|  3|\n",
      "|  4|  2|\n",
      "|  8|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "|RT @BTS_twt: Seou...| 54910|\n",
      "|\"RT @BTS_twt: 😱😱💜| 54180|\n",
      "|RT @realDonaldTru...| 19948|\n",
      "|RT @TheXFactor: W...| 16612|\n",
      "|RT @LeftAtLondon:...| 16298|\n",
      "|\"RT @BigHitEnt: S...| 14825|\n",
      "|RT @JoshDevineDru...| 14374|\n",
      "|\"RT @hendralm: Se...| 14233|\n",
      "|RT @PhilPrajya: ร...| 14006|\n",
      "|\"RT @Abdulaziz_Hm...| 13333|\n",
      "|RT @hellofeb22: ถ...| 13279|\n",
      "|\"RT @NCTsmtown_DR...| 12026|\n",
      "|RT @Lazy_and_lame...| 10438|\n",
      "|\"RT @Pantene: I G...| 10437|\n",
      "|RT @aserembaX: Na...|  9476|\n",
      "|RT @chartdata: .@...|  8936|\n",
      "|\"RT @Believe_In_L...|  8228|\n",
      "|RT @mistxxke: for...|  8096|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Menampilkan 2 tweet terbanyak:\n",
      "[('RT @LiamPayne: 9 years... Amazing to look back at all the memories and even now you got us trending worldwide after 9 years. You truly are…', 133763), ('RT @Zayg23: I cracked an egg and it was double yolked! I was excited so I recorded it on snap and asked google what it mean  ...  Should’ve…', 131023)]\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "|RT @BTS_twt: Seou...| 54910|\n",
      "|\"RT @BTS_twt: 😱😱💜| 54180|\n",
      "|RT @realDonaldTru...| 19948|\n",
      "|RT @TheXFactor: W...| 16612|\n",
      "|RT @LeftAtLondon:...| 16298|\n",
      "|\"RT @BigHitEnt: S...| 14825|\n",
      "|RT @JoshDevineDru...| 14374|\n",
      "|\"RT @hendralm: Se...| 14233|\n",
      "|RT @PhilPrajya: ร...| 14006|\n",
      "|\"RT @Abdulaziz_Hm...| 13333|\n",
      "|RT @hellofeb22: ถ...| 13279|\n",
      "|\"RT @NCTsmtown_DR...| 12026|\n",
      "|RT @Lazy_and_lame...| 10438|\n",
      "|\"RT @Pantene: I G...| 10437|\n",
      "|RT @aserembaX: Na...|  9476|\n",
      "|RT @chartdata: .@...|  8936|\n",
      "|\"RT @Believe_In_L...|  8228|\n",
      "|RT @mistxxke: for...|  8096|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Menampilkan 2 tweet terbanyak:\n",
      "+--------------------+------+\n",
      "|               Tweet|Jumlah|\n",
      "+--------------------+------+\n",
      "|RT @LiamPayne: 9 ...|133763|\n",
      "|RT @Zayg23: I cra...|131023|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('🧸💞 https://t.co/2Fxhd6GSSo', False), 1), (('🧸💕 https://t.co/xhnYD7ZAMW', False), 1), (('🧸ARMY are extremely dedicated and think #BTS is top tier! We really want to hear #SeoulTownRoad by @LilNasX feat RM… https://t.co/dAxEW2CCDm', False), 1), (('🧸 https://t.co/vZUSTP24D7', False), 1), (('🧸 https://t.co/rvTZSnE4OE', False), 1)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:\n",
    "# [1] Cholissodin, I., Riyandani, E., 2016, Analisis Big Data, Fakultas Ilmu Komputer, Universitas Brawijaya, Malang.\n",
    "# [2] Cholissodin, I. and Supianto, A. A. 2019. Enhancement Full Open Source Hadoop Distribution Universal Big Data Up Projects (UBig) From Education To Enterprise. 2019 International Conference on Sustainable Information Engineering and Technology (SIET), Lombok, Indonesia, 2019, pp. 90-93, doi: 10.1109/SIET48054.2019.8986040.\n",
    "# [3] Miao, K., Li, J., Hong, W. & Chen, M. 2020. A Microservice-Based Big Data Analysis Platform for Online Educational Applications. Annual Review of Anthropology, 2020, [ \"6929750\" ]. Available from: https://doi.org/10.1146/annurev.anthro.33.070203.144008\n",
    "# [4] Roy, S., et al. 2017. IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture. 2017 8th Annual Industrial Automation and Electromechanical Engineering Conference (IEMECON), Bangkok, 2017, pp. 303-304, doi: 10.1109/IEMECON.2017.8079610.\n",
    "# [5] Dabek, F. 2016. Leveraging Big Data to Provide a Web Service That Provides the Likelihood of Developing Psychological Conditions after a Concussion. 2016 IEEE International Conference on Mobile Services (MS), San Francisco, CA, 2016, pp. 160-165, doi: 10.1109/MobServ.2016.32.\n",
    "# [6] Andy A. 2016. Cloud Computing Part 5: SaaS (Software as a Service). Available at: https://andypi.co.uk/2016/05/23/cloud-computing-part-5-saas-software-as-a-service/ (Accessed on July 30, 2020).\n",
    "# [7] Putra, N. A., Putri, A. T., Prabowo, D. A., Surtiningsih, L., Arniantya, R., Cholissodin, I. 2017. Klasifikasi Sepeda Motor Berdasarkan Karakteristik Konsumen Dengan Metode K-Nearest Neighbour Pada Big Data Menggunakan Hadoop Single Node Cluster. Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK) FILKOM UB Vol. 4 No. 2, 81-86.\n",
    "# [8] Naveen, N. 2020. Hadoop MapReduce – Key Features & Highlights. Available at: https://intellipaat.com/blog/tutorial/big-data-and-hadoop-tutorial/hadoop-mapreduce-key-features-highlights/ (Accessed on March 8, 2020).\n",
    "# [9] stackchief. 2017. MapReduce Quick Explanation. Available at: https://www.stackchief.com/blog/MapReduce%20Quick%20Explanation (Accessed on March 8, 2020).\n",
    "# [10] .."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
